// Tencent is pleased to support the open source community by making ncnn available.
//
// Copyright (C) 2020 THL A29 Limited, a Tencent company. All rights reserved.
//
// Licensed under the BSD 3-Clause License (the "License"); you may not use this file except
// in compliance with the License. You may obtain a copy of the License at
//
// https://opensource.org/licenses/BSD-3-Clause
//
// Unless required by applicable law or agreed to in writing, software distributed
// under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR
// CONDITIONS OF ANY KIND, either express or implied. See the License for the
// specific language governing permissions and limitations under the License.

#version 450

#if NCNN_fp16_storage
#extension GL_EXT_shader_16bit_storage: require
struct sfpvec8 { f16vec4 abcd; f16vec4 efgh; };
#endif
#if NCNN_fp16_arithmetic
#extension GL_EXT_shader_explicit_arithmetic_types_float16: require
#endif

#extension GL_GOOGLE_include_directive: enable
#include "vulkan_activation.comp"

layout (constant_id = 0) const int kernel_w = 1;
layout (constant_id = 1) const int dilation_w = 1;
layout (constant_id = 2) const int stride_w = 1;
layout (constant_id = 3) const int bias_term = 0;
layout (constant_id = 4) const int activation_type = 0;
layout (constant_id = 5) const float activation_param_0 = 0;
layout (constant_id = 6) const float activation_param_1 = 0;

#define shape_constant_id_offset 7
layout (constant_id = shape_constant_id_offset + 0) const int dims = 0;
layout (constant_id = shape_constant_id_offset + 1) const int w = 0;
layout (constant_id = shape_constant_id_offset + 2) const int h = 0;
layout (constant_id = shape_constant_id_offset + 3) const int c = 0;
layout (constant_id = shape_constant_id_offset + 4) const int cstep = 0;

layout (constant_id = shape_constant_id_offset + 5) const int outdims = 0;
layout (constant_id = shape_constant_id_offset + 6) const int outw = 0;
layout (constant_id = shape_constant_id_offset + 7) const int outh = 0;
layout (constant_id = shape_constant_id_offset + 8) const int outc = 0;
layout (constant_id = shape_constant_id_offset + 9) const int outcstep = 0;

#if NCNN_image_shader
layout (binding = 0) uniform unfp sampler3D bottom_blob;
layout (binding = 1, imfmtc4) writeonly uniform unfp image3D top_blob;
layout (binding = 2) uniform unfp sampler3D weight_blob;
layout (binding = 3) uniform unfp sampler3D bias_blob;
#else
layout (binding = 0) readonly buffer bottom_blob { sfpvec8 bottom_blob_data[]; };
layout (binding = 1) writeonly buffer top_blob { sfpvec8 top_blob_data[]; };
layout (binding = 2) readonly buffer weight_blob { sfpvec8 weight_data[]; };
layout (binding = 3) readonly buffer bias_blob { sfpvec8 bias_data[]; };
#endif

layout (push_constant) uniform parameter
{
    int dims;
    int w;
    int h;
    int c;
    int cstep;

    int outdims;
    int outw;
    int outh;
    int outc;
    int outcstep;
} p;

void main()
{
    int gx = int(gl_GlobalInvocationID.x);
    int gy = int(gl_GlobalInvocationID.y);
    int gz = int(gl_GlobalInvocationID.z);

    if (gx >= psc(outw) || gy >= psc(outh) || gz >= psc(outc))
        return;

    afpvec8 sum;

    if (bias_term == 1)
    {
#if NCNN_image_shader
        sum = image3d_ld8(bias_blob, ivec3(gy, 0, 0));
#else
        sum = buffer_ld8(bias_data, gy);
#endif
    }
    else
    {
        sum = afpvec8(afpvec4(0.f), afpvec4(0.f));
    }

#if NCNN_image_shader
    int ymax = psc(h); // num of input channels
    int xx = gx / stride_w;  // input block index
    int widx = gx - xx * stride_w; // kernel index equivalent current input

    for (; (widx < kernel_w) && (xx >= 0); widx += stride_w, --xx)
    {
        for (int y = 0; y < ymax; ++y)
        {
            afpvec8 v = image3d_ld8(bottom_blob, ivec3(xx, y, gz));

            afpvec8 k0 = image3d_ld8(weight_blob, ivec3(widx * 8 + 0, y, gy));
            afpvec8 k1 = image3d_ld8(weight_blob, ivec3(widx * 8 + 1, y, gy));
            afpvec8 k2 = image3d_ld8(weight_blob, ivec3(widx * 8 + 2, y, gy));
            afpvec8 k3 = image3d_ld8(weight_blob, ivec3(widx * 8 + 3, y, gy));
            afpvec8 k4 = image3d_ld8(weight_blob, ivec3(widx * 8 + 4, y, gy));
            afpvec8 k5 = image3d_ld8(weight_blob, ivec3(widx * 8 + 5, y, gy));
            afpvec8 k6 = image3d_ld8(weight_blob, ivec3(widx * 8 + 6, y, gy));
            afpvec8 k7 = image3d_ld8(weight_blob, ivec3(widx * 8 + 7, y, gy));

            // sum += v * k
            sum[0].r += dot(v[0], k0[0]) + dot(v[1], k0[1]);
            sum[0].g += dot(v[0], k1[0]) + dot(v[1], k1[1]);
            sum[0].b += dot(v[0], k2[0]) + dot(v[1], k2[1]);
            sum[0].a += dot(v[0], k3[0]) + dot(v[1], k3[1]);
            sum[1].r += dot(v[0], k4[0]) + dot(v[1], k4[1]);
            sum[1].g += dot(v[0], k5[0]) + dot(v[1], k5[1]);
            sum[1].b += dot(v[0], k6[0]) + dot(v[1], k6[1]);
            sum[1].a += dot(v[0], k7[0]) + dot(v[1], k7[1]);
        }
    }

#else
    int ymax = psc(h); // num of input channels
    int xmax = psc(w); // input size
    int xx = gx / stride_w;  // input block index
    int widx = gx - xx * stride_w; // kernel index equivalent current input

    int w_offset_0 = kernel_w * ymax * gy + widx;
    
    for (; (widx < kernel_w) && (xx >= 0); widx += stride_w, --xx)
    {
        int v_offset = xx;
        int w_offset = w_offset_0;

        for (int y = 0; y < ymax; ++y)
        {
            afpvec8 v = buffer_ld8(bottom_blob_data, v_offset);

            afpvec8 k0 = buffer_ld8(weight_data, w_offset * 8 + 0);
            afpvec8 k1 = buffer_ld8(weight_data, w_offset * 8 + 1);
            afpvec8 k2 = buffer_ld8(weight_data, w_offset * 8 + 2);
            afpvec8 k3 = buffer_ld8(weight_data, w_offset * 8 + 3);
            afpvec8 k4 = buffer_ld8(weight_data, w_offset * 8 + 4);
            afpvec8 k5 = buffer_ld8(weight_data, w_offset * 8 + 5);
            afpvec8 k6 = buffer_ld8(weight_data, w_offset * 8 + 6);
            afpvec8 k7 = buffer_ld8(weight_data, w_offset * 8 + 7);

            // sum += v * k
            sum[0].r += dot(v[0], k0[0]) + dot(v[1], k0[1]);
            sum[0].g += dot(v[0], k1[0]) + dot(v[1], k1[1]);
            sum[0].b += dot(v[0], k2[0]) + dot(v[1], k2[1]);
            sum[0].a += dot(v[0], k3[0]) + dot(v[1], k3[1]);
            sum[1].r += dot(v[0], k4[0]) + dot(v[1], k4[1]);
            sum[1].g += dot(v[0], k5[0]) + dot(v[1], k5[1]);
            sum[1].b += dot(v[0], k6[0]) + dot(v[1], k6[1]);
            sum[1].a += dot(v[0], k7[0]) + dot(v[1], k7[1]);

            v_offset += xmax;
            w_offset += kernel_w;
        }

        w_offset_0 += stride_w;
    }

#endif

    sum = activation_afpvec8(sum, activation_type, activation_param_0, activation_param_1);

#if NCNN_image_shader
    image3d_st8(top_blob, ivec3(gx, gy, gz), sum);
#else
    const int gi = gz * psc(outcstep) + gy * psc(outw) + gx;

    buffer_st8(top_blob_data, gi, sum);
#endif
}
